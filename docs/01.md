以下では、**最小限(MVP)の形**で、提示されたJSONログ（行ごとに1レコード）を**RedshiftでDWH化**するために行うステップを示します。テーブル定義からCOPY文まで、できるだけシンプルにまとめました。

---

# 1. ログファイル (サンプル)

例として `sample-logs.jsonl` というファイルに以下の内容が入っているとします:

```json
{"timestamp": "2023-03-28T12:00:01Z", "level": "INFO",  "message": "User login",         "userId": 101, "sessionId": "abc123", "ipAddress": "192.168.0.10"}
{"timestamp": "2023-03-28T12:00:05Z", "level": "INFO",  "message": "Dashboard viewed",  "userId": 101, "sessionId": "abc123", "ipAddress": "192.168.0.10"}
{"timestamp": "2023-03-28T12:00:15Z", "level": "ERROR", "message": "Payment failed",     "userId": 102, "sessionId": "xyz890", "errorCode": 500, "ipAddress": "192.168.0.22"}
{"timestamp": "2023-03-28T12:00:20Z", "level": "INFO",  "message": "User logout",        "userId": 101, "sessionId": "abc123", "ipAddress": "192.168.0.10"}
{"timestamp": "2023-03-28T12:00:25Z", "level": "WARN",  "message": "Session timed out",  "userId": 103, "sessionId": "pqr456", "ipAddress": "192.168.0.33"}
```

各行が1つのJSONオブジェクトとなっており、以下のキーがあります:

- `timestamp` (文字列, ISO8601形式)
- `level` (文字列: `"INFO"`, `"ERROR"`, `"WARN"`など)
- `message` (文字列: ログ本文)
- `userId` (数値: ユーザーID)
- `sessionId` (文字列: セッションID)
- `ipAddress` (文字列: IPアドレス)
- `errorCode` (数値: 一部の行のみ)

---

# 2. S3 にアップロード

`sample-logs.jsonl` を S3 に置きます（MVPのため、手動アップロードでもOK）。

```bash
aws s3 cp sample-logs.jsonl s3://custum-log-bucket-01/logs/sample-logs.jsonl
```

> `s3://custum-log-bucket-01/logs/` の部分は好きなパスに調整してください。

---

# 3. Redshift でテーブル定義

Redshift Serverlessでもクラスタでも可) に接続し、ログを受け取るテーブルを作ります。  
MVPとしては、下記の単純なスキーマを想定:

```sql
CREATE TABLE event_log (
    "timestamp" TIMESTAMP,
    "level"     VARCHAR(10),
    "message"   VARCHAR(255),
    "userId"    INT,
    "sessionId" VARCHAR(50),
    "ipAddress" VARCHAR(50),
    "errorCode" INT
);
```

- `"timestamp"` → TIMESTAMP型  
- `"level"` → INFO, ERROR等が入るので VARCHAR(10)程度  
- `"message"` → 簡単なログ本文を想定 (255文字)  
- `"userId"` → INT  
- `"sessionId"` → 一旦50文字  
- `"ipAddress"` → 50文字  
- `"errorCode"` → ない行もあるのでNULL許可 (INT型)

---

# 4. COPY 文で JSON をロード (MVP)

Redshift (またはServerless) 上で COPY文を実行します。**`FORMAT AS JSON 'auto'`** とすることで、列名に対応したキーを自動的にマッピングしてくれます。

```sql
COPY event_log
FROM 's3://custum-log-bucket-01/logs/sample-logs.jsonl'
IAM_ROLE 'arn:aws:iam::<ACCOUNT_ID>:role/RedshiftCopyRole'
FORMAT AS JSON 'auto'
TIMEFORMAT 'auto'
REGION 'ap-northeast-1';
```

- `s3://custum-log-bucket-01/logs/sample-logs.jsonl` → S3の実パスに置き換え
- `IAM_ROLE 'arn:aws:iam::<ACCOUNT_ID>:role/RedshiftCopyRole'` → RedshiftがS3を読み取れるIAMロールを設定  
- `FORMAT AS JSON 'auto'` → JSONキーとテーブル列の名前が一致すると自動マッピング
- `TIMEFORMAT 'auto'` → `timestamp`フィールドをタイムスタンプとして正しくパース
- `REGION 'ap-northeast-1'` → 使用リージョンに合わせる

実行後、 `SELECT * FROM event_log;` でデータが取り込めているか確認。

---

## 4.1 JSONキーと列名が一致しない場合

- `FORMAT AS JSON 'auto'` は**テーブル列とJSONキーが正確に一致**している必要があり、追加で存在するキーは無視される。  
- もしキー名を変えたい or より複雑なマッピングをしたい場合は、**JSONPathsファイル**を用意し `FORMAT AS JSON 's3://.../myjsonpaths.json'` のように指定する必要がある。  
- MVPならキー名を合わせるだけでも十分です。

---

# 5. 確認 & 拡張

1. **データ確認**  
   ```sql
   SELECT *
   FROM event_log
   ORDER BY "timestamp";
   ```
   - "timestamp"や"level"、"errorCode"のNULLなどが正しく入っているかを見る。

2. **データ活用**  
   - ダッシュボード: QuickSightからRedshiftに接続し、`event_log` を可視化  
   - 集計例:  
     ```sql
     SELECT level, COUNT(*) as cnt
     FROM event_log
     GROUP BY level
     ORDER BY cnt DESC;
     ```
   - "errorCode"の平均や、ユーザごとのログ頻度を見る等も自由にやってみる。

3. **Auto COPY** (オプション)  
   - Redshift Serverlessなら**Auto ingestion (Auto COPY)**を設定し、`s3://custum-log-bucket-01/logs/` 配下の新ファイルを自動取り込みにする。  
   - そうすれば手動で COPY 文を叩かなくても新ファイルをロード可能。

---

# 6. まとめ

1. **最小限(MVP)ログ**は、上記のように**行ごとのJSON**をS3に置く。  
2. Redshiftに**テーブルを作り**、**COPY**文(`FORMAT AS JSON 'auto'`)で簡易マッピングを利用すれば、ログが取り込める。  
3. 後から**列を追加**したい、**より大規模に管理**したい場合は、パーティションフォルダやJSONPathsファイルを検討。  
4. **Auto COPY**を使えば手動COPYすら不要になる。  

この手順であれば**一番シンプルに「S3上のJSON→Redshift DWH」**を実現できるので、MVP段階の速度重視に最適です。ぜひ試してみてください。